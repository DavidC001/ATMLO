exp_name: "ACDC"

seed: 42

out_dir: "results"

data_preprocessing:
  dataset_dir: "datasets"
  global_padding: False

  input_jsons: [
    "datasets/LogicBench/LogicBench(Aug)/propositional_logic/modus_tollens/data.json",
    "datasets/LogicBench/LogicBench(Eval)/BQA/propositional_logic/modus_tollens/data.json",
  ]

  model_names: [
    "EleutherAI/pythia-14m",
    "gpt2-small",
    "EleutherAI/pythia-410m",
    "Qwen/Qwen2.5-0.5B-Instruct",
    "Qwen/Qwen2.5-1.5B-Instruct",
    "meta-llama/Llama-3.2-1B-Instruct",
    "gpt2",
  ]

  templates: [
    "alpaca",
    "alpaca",
    "alpaca",
    "qwen",
    "qwen",
    "llama_3",
    "alpaca",
  ]


model:
  model_name: "meta-llama/Llama-3.2-1B-Instruct"
  # model_name: "gpt2-small"
  # model_name: "gpt2-xl"
  # model_name: 'EleutherAI/pythia-410m'
  # model_name: 'EleutherAI/pythia-14m'
  # model_name: 'bigscience/bloom-1b1'
  # model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  # model_name: "Qwen/Qwen2.5-0.5B-Instruct"
  device: "cuda"
  
  batch_size: 8
  train_percent: 0.8

  threshold: 5e-3

  tokenGraph: False

  method: "mask_gradient" # "mask_gradient" or "ACDC" or "edge_attribution_patching"

  tao_exps: [-2]
  tao_bases: [5]
